{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "BERT_GAP_Model.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dowloading Dataset And Importing Required Libraries"
      ],
      "metadata": {
        "id": "WPHf_vlY0zMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-development.tsv -q\n",
        "!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-test.tsv -q\n",
        "!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-validation.tsv -q"
      ],
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.status.busy": "2022-05-04T09:53:28.043584Z",
          "iopub.execute_input": "2022-05-04T09:53:28.043890Z",
          "iopub.status.idle": "2022-05-04T09:53:33.147143Z",
          "shell.execute_reply.started": "2022-05-04T09:53:28.043830Z",
          "shell.execute_reply": "2022-05-04T09:53:33.146037Z"
        },
        "trusted": true,
        "id": "6uJ9I3BO0zME"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "execution": {
          "iopub.status.busy": "2022-05-04T09:53:33.149157Z",
          "iopub.execute_input": "2022-05-04T09:53:33.149537Z",
          "iopub.status.idle": "2022-05-04T09:53:39.521408Z",
          "shell.execute_reply.started": "2022-05-04T09:53:33.149478Z",
          "shell.execute_reply": "2022-05-04T09:53:39.520220Z"
        },
        "trusted": true,
        "id": "IusGU7ZM0zMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33de430c-8ae9-4b9b-e85a-9df126b9989e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 19.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61 kB 3.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 123 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.22.6-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 11.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.2.0)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting botocore<1.26.0,>=1.25.6\n",
            "  Downloading botocore-1.25.6-py3-none-any.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 9.4 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 39.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.26.0,>=1.25.6->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.26.0,>=1.25.6->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2021.10.8)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 37.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.22.6 botocore-1.25.6 jmespath-1.0.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.5.2 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn import Module, Linear, Dropout\n",
        "import torch.nn.functional as F\n",
        "from pytorch_pretrained_bert.modeling import BertModel, BertLayer\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert.optimization import BertAdam\n",
        "from pytorch_pretrained_bert.optimization import WarmupLinearSchedule\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import RandomSampler\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T09:53:39.523493Z",
          "iopub.execute_input": "2022-05-04T09:53:39.523864Z",
          "iopub.status.idle": "2022-05-04T09:53:39.534514Z",
          "shell.execute_reply.started": "2022-05-04T09:53:39.523804Z",
          "shell.execute_reply": "2022-05-04T09:53:39.533155Z"
        },
        "trusted": true,
        "id": "Jiil5TdG0zMG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 9876\n",
        "\n",
        "#seed function is used to generate same random numbers \n",
        "#again and again and simplifies algorithm testing process.\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "np.random.seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "#reproducibility of the code\n",
        "#control sources of randomness that can cause multiple \n",
        "#executions of your application to behave differently\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T09:53:39.536203Z",
          "iopub.execute_input": "2022-05-04T09:53:39.536573Z",
          "iopub.status.idle": "2022-05-04T09:53:39.549387Z",
          "shell.execute_reply.started": "2022-05-04T09:53:39.536514Z",
          "shell.execute_reply": "2022-05-04T09:53:39.548329Z"
        },
        "trusted": true,
        "id": "quKrGhVf0zMH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing Model And Other Pertaining Values"
      ],
      "metadata": {
        "id": "Yv5UgNHPXsdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "bert_model = \"bert-base-cased\"\n",
        "n_bertlayers = 12\n",
        "dropout = 0.1\n",
        "\n",
        "# Preprocessing\n",
        "do_lower_case = False\n",
        "\n",
        "# Training\n",
        "train_batch_size = 4\n",
        "#Gradient accumulation is a mechanism to split the batch of samples — used for training a neural network — into \n",
        "# several mini-batches of samples that will be run sequentially.\n",
        "gradient_accumulation_steps = 5\n",
        "lr = 1e-5\n",
        "num_train_epochs = 5\n",
        "#Warm-up is a way to reduce the primacy effect of the early training examples. \n",
        "#here i run it vause i am only able to run for 5 epochs, so prevent early overfitting \n",
        "#and using mroe epochs to train the data\n",
        "warmup_proportion = 0.1\n",
        "optim = \"bertadam\"\n",
        "weight_decay = False\n",
        "\n",
        "\n",
        "# Others\n",
        "eval_batch_size = 32\n",
        "device = torch.device(\"cuda\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T09:53:39.551358Z",
          "iopub.execute_input": "2022-05-04T09:53:39.552081Z",
          "iopub.status.idle": "2022-05-04T09:53:39.561536Z",
          "shell.execute_reply.started": "2022-05-04T09:53:39.551822Z",
          "shell.execute_reply": "2022-05-04T09:53:39.560479Z"
        },
        "trusted": true,
        "id": "mrunO-S00zMI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Handling"
      ],
      "metadata": {
        "id": "O-s7rnlCX04E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_tag(row):\n",
        "    #Insert custom tags in the sentence find the position of A, B, and the pronoun after tokenization.\n",
        "    to_be_inserted = sorted([(row[\"A-offset\"], \" [A] \"),(row[\"B-offset\"], \" [B] \"),(row[\"Pronoun-offset\"], \" [P] \")], key=lambda x: x[0], reverse=True)\n",
        "    text = row[\"Text\"]\n",
        "    for offset, tag in to_be_inserted:\n",
        "        text = text[:int(offset)] + tag + text[int(offset):]\n",
        "    return text\n",
        "\n",
        "def tokenize(text, tokenizer):\n",
        "    #Returns a list of tokens and the positions of A, B, and the Pronoun.\n",
        "    entries = {}\n",
        "    final_tokens = []\n",
        "    for token in tokenizer.tokenize(text):\n",
        "        if token in (\"[A]\", \"[B]\", \"[P]\"):\n",
        "            entries[token] = len(final_tokens)\n",
        "            continue\n",
        "        final_tokens.append(token)\n",
        "    return final_tokens, (entries[\"[A]\"], entries[\"[B]\"], entries[\"[P]\"])\n",
        "\n",
        "\n",
        "class GAPDataset(Dataset):\n",
        "    #Custom GAP Dataset class\n",
        "    def __init__(self, df, tokenizer, labeled=True):\n",
        "        self.labeled = labeled\n",
        "        if labeled:\n",
        "            tmp = df[[\"A-coref\", \"B-coref\"]].copy()\n",
        "            tmp[\"Neither\"] = ~(df[\"A-coref\"] | df[\"B-coref\"])\n",
        "            self.y = tmp.values.astype(\"bool\")\n",
        "        # Extracts the tokens and offsets(positions of A, B, and P)\n",
        "        self.offsets, self.tokens = [], []\n",
        "        self.seq_len = []\n",
        "        for _, row in df.iterrows():\n",
        "            text = insert_tag(row)\n",
        "            tokens, offsets = tokenize(text, tokenizer)\n",
        "            self.offsets.append(offsets)\n",
        "            self.tokens.append(tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens + [\"[SEP]\"]))\n",
        "            self.seq_len.append(len(self.tokens[-1]))\n",
        "    def __len__(self):\n",
        "        return len(self.tokens)\n",
        "    def __getitem__(self, idx):\n",
        "        if self.labeled:\n",
        "            return self.tokens[idx], self.offsets[idx], self.y[idx]\n",
        "        else:\n",
        "            return self.tokens[idx], self.offsets[idx]\n",
        "    def get_seq_len(self):\n",
        "        return self.seq_len\n",
        "\n",
        "\n",
        "def collate_examples(batch, truncate_len=500):\n",
        "    \"\"\"we do batch preparation where, we:\n",
        "    1. do padding\n",
        "    2. convert to tensors\n",
        "    \"\"\"\n",
        "    transposed = list(zip(*batch))\n",
        "    max_len = min(max((len(x) for x in transposed[0])),truncate_len)\n",
        "    tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n",
        "    for i, row in enumerate(transposed[0]):\n",
        "        row = np.array(row[:truncate_len])\n",
        "        tokens[i, :len(row)] = row\n",
        "    token_tensor = torch.from_numpy(tokens)\n",
        "    # Offsets\n",
        "    offsets = torch.stack([torch.LongTensor(x) for x in transposed[1]], dim=0) + 1 # Account for the [CLS] token\n",
        "    # Labels\n",
        "    if len(transposed) == 2:\n",
        "        return token_tensor, offsets, None\n",
        "    #create one-hot encodes the outputs \n",
        "    #0,0,0 or 1,0,0 or 0,0,1 etc.\n",
        "    one_hot_labels = torch.stack([torch.from_numpy(x.astype(\"uint8\")) for x in transposed[2]], dim=0)\n",
        "    _, labels = one_hot_labels.max(dim=1)\n",
        "    return token_tensor, offsets, labels"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T09:53:39.564501Z",
          "iopub.execute_input": "2022-05-04T09:53:39.565559Z",
          "iopub.status.idle": "2022-05-04T09:53:39.585131Z",
          "shell.execute_reply.started": "2022-05-04T09:53:39.564981Z",
          "shell.execute_reply": "2022-05-04T09:53:39.583651Z"
        },
        "trusted": true,
        "id": "j5CrDmW00zMK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "Kq09B5AOX6Va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pretrained_bert(modelname, num_hidden_layers=None):\n",
        "    bert = BertModel.from_pretrained(modelname)\n",
        "    return bert\n",
        "\n",
        "class BertCl_GAP(Module):\n",
        "    #model definition\n",
        "    def __init__(self, bert, dropout, n_offsets=3):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.bert_hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = Dropout(dropout)\n",
        "        self.classifier = Linear(self.bert.config.hidden_size * n_offsets, n_offsets)\n",
        "    #forward propagate\n",
        "    def forward(self, token_tensor, offsets, label_id=None):\n",
        "        bert_outputs, _ = self.bert(\n",
        "            token_tensor, attention_mask=(token_tensor > 0).long(),\n",
        "            token_type_ids=None, output_all_encoded_layers=False)\n",
        "        extracted_outputs = bert_outputs.gather(1, offsets.unsqueeze(2).expand(-1, -1, bert_outputs.size(2))).view(bert_outputs.size(0), -1)\n",
        "        outputs = self.classifier(self.dropout(extracted_outputs))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "def run_epoch(model, dataloader, optimizer, criterion, device,verbose_step=10000):\n",
        "    model.train()\n",
        "    t1 = time.time()\n",
        "    tr_loss = 0\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        label_ids = batch[-1]\n",
        "        outputs = model(*batch[:-1])\n",
        "        if criterion._get_name() == \"BCEWithLogitsLoss\":\n",
        "            outputs = outputs[:, 0]\n",
        "            label_ids = label_ids.float()\n",
        "        loss = criterion(outputs, label_ids)\n",
        "        if gradient_accumulation_steps > 1:\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "        loss.backward()\n",
        "        tr_loss += loss.item()\n",
        "        if (step + 1) % verbose_step == 0:\n",
        "            loss_now = gradient_accumulation_steps * tr_loss / (step + 1)\n",
        "            print(f'step:{step+1} loss:{loss_now:.7f} time:{time.time() - t1:.1f}s')\n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            model.zero_grad()\n",
        "    return gradient_accumulation_steps * tr_loss / (step + 1)\n",
        "\n",
        "\n",
        "def predict(model, data_loader, device):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    for step, batch in enumerate(data_loader):\n",
        "        batch = batch[:2]\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            logits = model(*batch)\n",
        "            preds.append(logits.detach().cpu())\n",
        "    preds = torch.cat(preds) if len(preds) > 1 else preds[0]\n",
        "    if preds.size(-1) > 1:\n",
        "        preds = F.softmax(preds, dim=1)\n",
        "    else:\n",
        "        preds = torch.sigmoid(preds)\n",
        "    return preds.numpy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T09:53:39.587841Z",
          "iopub.execute_input": "2022-05-04T09:53:39.588608Z",
          "iopub.status.idle": "2022-05-04T09:53:39.609093Z",
          "shell.execute_reply.started": "2022-05-04T09:53:39.588337Z",
          "shell.execute_reply": "2022-05-04T09:53:39.607809Z"
        },
        "trusted": true,
        "id": "k0r2dj7t0zML"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gap_model(bert_model, n_bertlayers, dropout,steps_per_epoch, device):\n",
        "    bert = get_pretrained_bert(bert_model, n_bertlayers)\n",
        "#     bert = BertModel.from_pretrained(modelname)\n",
        "    model = BertCl_GAP(bert, dropout)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "\n",
        "    if weight_decay:\n",
        "        no_decay = [\"bias\", \"gamma\", \"beta\", \"head\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "             \"weight_decay\": 0.01},\n",
        "            {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "             \"weight_decay\": 0.0}\n",
        "        ]\n",
        "    else:\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\"params\": [p for n, p in param_optimizer], \"weight_decay\": 0.0}\n",
        "\n",
        "        ]\n",
        "\n",
        "    t_total = int(\n",
        "        steps_per_epoch / gradient_accumulation_steps * num_train_epochs)\n",
        "    if optim == 'bertadam':\n",
        "        optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                             lr=lr,\n",
        "                             warmup=warmup_proportion,\n",
        "                             t_total=t_total)\n",
        "    return model, optimizer\n",
        "\n",
        "\n",
        "def get_loader(train_df, val_df, test_df):\n",
        "    tokenizer = BertTokenizer.from_pretrained(\n",
        "        bert_model,\n",
        "        do_lower_case=do_lower_case,\n",
        "        never_split=(\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\", \"[A]\", \"[B]\", \"[P]\")\n",
        "    )\n",
        "    # These tokens are not actually used, so we can assign arbitrary values.\n",
        "    tokenizer.vocab['[A]'] = -1\n",
        "    tokenizer.vocab['[B]'] = -1\n",
        "    tokenizer.vocab['[P]'] = -1\n",
        "\n",
        "    train_ds = GAPDataset(train_df, tokenizer)\n",
        "    val_ds = GAPDataset(val_df, tokenizer)\n",
        "    test_ds = GAPDataset(test_df, tokenizer, labeled=False)\n",
        "        \n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        collate_fn=collate_examples,\n",
        "        batch_size=train_batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        collate_fn=collate_examples,\n",
        "        batch_size=32,\n",
        "        shuffle=False\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_ds,\n",
        "        collate_fn=collate_examples,\n",
        "        batch_size=32,\n",
        "        shuffle=False\n",
        "    )\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T09:53:39.612751Z",
          "iopub.execute_input": "2022-05-04T09:53:39.613376Z",
          "iopub.status.idle": "2022-05-04T09:53:39.630400Z",
          "shell.execute_reply.started": "2022-05-04T09:53:39.613316Z",
          "shell.execute_reply": "2022-05-04T09:53:39.629110Z"
        },
        "trusted": true,
        "id": "MjsAEED_0zMM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading DataLoaders"
      ],
      "metadata": {
        "id": "z5ycme2zX9ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df = pd.concat([pd.read_csv(data_dir + \"gap-test.tsv\", delimiter=\"\\t\"),\n",
        "#                       pd.read_csv(data_dir + \"gap-development.tsv\", delimiter=\"\\t\")])\n",
        "train_df = pd.read_csv(\"gap-development.tsv\", delimiter=\"\\t\")\n",
        "val_df = pd.read_csv(\"gap-validation.tsv\", delimiter=\"\\t\")\n",
        "val_y = val_df[['A-coref', 'B-coref']].astype(int)\n",
        "val_y['None'] = 1 - val_y.sum(1)\n",
        "\n",
        "test_df = pd.read_csv(\"gap-test.tsv\", delimiter=\"\\t\")\n",
        "test_df.drop(\"A-coref\", axis=1, inplace=True)\n",
        "test_df.drop(\"B-coref\", axis=1, inplace=True)\n",
        "\n",
        "print(f\"Train:{train_df.shape[0]}, Valid:{val_df.shape[0]}, Test:{test_df.shape[0]}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T09:53:39.632384Z",
          "iopub.execute_input": "2022-05-04T09:53:39.633035Z",
          "iopub.status.idle": "2022-05-04T09:53:39.707231Z",
          "shell.execute_reply.started": "2022-05-04T09:53:39.632941Z",
          "shell.execute_reply": "2022-05-04T09:53:39.706091Z"
        },
        "trusted": true,
        "id": "mv1mlC8i0zMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab79f8d4-6049-4836-f111-83bd6630ab98"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:2000, Valid:454, Test:2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader, test_loader = get_loader(train_df, val_df, test_df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T09:53:39.708879Z",
          "iopub.execute_input": "2022-05-04T09:53:39.709589Z",
          "iopub.status.idle": "2022-05-04T09:53:50.863835Z",
          "shell.execute_reply.started": "2022-05-04T09:53:39.709531Z",
          "shell.execute_reply": "2022-05-04T09:53:50.862806Z"
        },
        "trusted": true,
        "id": "_9Lc-Ele0zMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa23a59a-5250-4fe3-92f8-158e256157fb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 213450/213450 [00:00<00:00, 2442000.98B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "steps_per_epoch = len(train_loader)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T09:53:50.865538Z",
          "iopub.execute_input": "2022-05-04T09:53:50.865869Z",
          "iopub.status.idle": "2022-05-04T09:53:50.871604Z",
          "shell.execute_reply.started": "2022-05-04T09:53:50.865812Z",
          "shell.execute_reply": "2022-05-04T09:53:50.870450Z"
        },
        "trusted": true,
        "id": "iBAlCSg90zMO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T09:53:50.873679Z",
          "iopub.execute_input": "2022-05-04T09:53:50.874461Z",
          "iopub.status.idle": "2022-05-04T09:53:50.883453Z",
          "shell.execute_reply.started": "2022-05-04T09:53:50.874391Z",
          "shell.execute_reply": "2022-05-04T09:53:50.882505Z"
        },
        "trusted": true,
        "id": "y1EJ2tGS0zMO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "k94GUU0sYBcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, optimizer = get_gap_model(bert_model, n_bertlayers, dropout,\n",
        "                                 steps_per_epoch, device)\n",
        "print(\"Starting Training \\n\")\n",
        "for e in range(num_train_epochs):\n",
        "    t1 = time.time()\n",
        "    tr_loss = run_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_pr = predict(model, val_loader, device)\n",
        "    val_loss = log_loss(val_y, val_pr)\n",
        "    elapsed = time.time() - t1\n",
        "    print(f\"Epoch:{e + 1} tr_loss:{tr_loss:.4f} val_loss:{val_loss:.4f}\"\n",
        "          f\" Time:{elapsed:.1f}s\")\n",
        "    scores.append({\"model_id\": 1, \"epoch\": e + 1, \"time\": elapsed,\n",
        "                   \"tr_loss\": tr_loss, \"val_loss\": val_loss})\n",
        "test_pr = predict(model, test_loader, device)\n",
        "torch.save(model, 'model.pth')\n",
        "\n",
        "# del model, optimizer\n",
        "# torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T09:53:50.885594Z",
          "iopub.execute_input": "2022-05-04T09:53:50.885863Z",
          "iopub.status.idle": "2022-05-04T09:58:24.929168Z",
          "shell.execute_reply.started": "2022-05-04T09:53:50.885807Z",
          "shell.execute_reply": "2022-05-04T09:58:24.927860Z"
        },
        "trusted": true,
        "id": "HdkkLGSm0zMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a62fac42-ac99-4482-cd54-23577244f07c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 404400730/404400730 [00:27<00:00, 14842017.63B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1055.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1 tr_loss:0.7243 val_loss:0.5159 Time:139.5s\n",
            "Epoch:2 tr_loss:0.2874 val_loss:0.4340 Time:138.5s\n",
            "Epoch:3 tr_loss:0.0928 val_loss:0.5244 Time:138.1s\n",
            "Epoch:4 tr_loss:0.0468 val_loss:0.5455 Time:138.5s\n",
            "Epoch:5 tr_loss:0.0308 val_loss:0.5606 Time:138.4s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = torch.load('model.pth')\n",
        "test_pr = predict(model, test_loader, device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T10:00:21.918635Z",
          "iopub.execute_input": "2022-05-04T10:00:21.918959Z",
          "iopub.status.idle": "2022-05-04T10:00:35.907373Z",
          "shell.execute_reply.started": "2022-05-04T10:00:21.918898Z",
          "shell.execute_reply": "2022-05-04T10:00:35.906339Z"
        },
        "trusted": true,
        "id": "fQcCHSfN0zMP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Results"
      ],
      "metadata": {
        "id": "kFL9_PvGYFgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(scores)\n",
        "\n",
        "pd.set_option(\"precision\", 5)\n",
        "print(\"\\nSingle model\")\n",
        "print(df.groupby(\"epoch\")[['tr_loss', 'val_loss']].mean())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T10:00:52.460986Z",
          "iopub.execute_input": "2022-05-04T10:00:52.461661Z",
          "iopub.status.idle": "2022-05-04T10:00:52.483885Z",
          "shell.execute_reply.started": "2022-05-04T10:00:52.461382Z",
          "shell.execute_reply": "2022-05-04T10:00:52.482821Z"
        },
        "trusted": true,
        "id": "tvpBSL760zMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "140dae72-7aa5-490c-a7de-cf3d37ffed4a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Single model\n",
            "       tr_loss  val_loss\n",
            "epoch                   \n",
            "1      0.72429   0.51591\n",
            "2      0.28744   0.43397\n",
            "3      0.09281   0.52441\n",
            "4      0.04677   0.54554\n",
            "5      0.03082   0.56056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_pr_avg /= n_models\n",
        "# test_pr_avg /= 1\n",
        "df_sub = pd.DataFrame(test_pr, columns=[\"A\", \"B\", \"NEITHER\"])\n",
        "df_sub[\"ID\"] = test_df.ID\n",
        "df_sub.to_csv(\"submission.csv\", index=False)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T10:00:53.045782Z",
          "iopub.execute_input": "2022-05-04T10:00:53.046400Z",
          "iopub.status.idle": "2022-05-04T10:00:53.072671Z",
          "shell.execute_reply.started": "2022-05-04T10:00:53.046171Z",
          "shell.execute_reply": "2022-05-04T10:00:53.071172Z"
        },
        "trusted": true,
        "id": "-1ZRhXdX0zMQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T10:00:53.302777Z",
          "iopub.execute_input": "2022-05-04T10:00:53.303172Z",
          "iopub.status.idle": "2022-05-04T10:00:53.344111Z",
          "shell.execute_reply.started": "2022-05-04T10:00:53.303111Z",
          "shell.execute_reply": "2022-05-04T10:00:53.342850Z"
        },
        "trusted": true,
        "id": "CcUPskfi0zMQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "09456c91-975d-446c-a1c9-c282cae6d9a2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            A        B  NEITHER         ID\n",
              "0     0.00038  0.99711  0.00251     test-1\n",
              "1     0.99862  0.00027  0.00111     test-2\n",
              "2     0.99726  0.00037  0.00237     test-3\n",
              "3     0.00214  0.99332  0.00454     test-4\n",
              "4     0.99842  0.00019  0.00139     test-5\n",
              "...       ...      ...      ...        ...\n",
              "1995  0.99870  0.00013  0.00118  test-1996\n",
              "1996  0.99711  0.00068  0.00221  test-1997\n",
              "1997  0.97686  0.00038  0.02276  test-1998\n",
              "1998  0.99390  0.00366  0.00244  test-1999\n",
              "1999  0.99807  0.00011  0.00182  test-2000\n",
              "\n",
              "[2000 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-af2a220d-0ab6-4d36-a06e-7cf914c8261d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>NEITHER</th>\n",
              "      <th>ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00038</td>\n",
              "      <td>0.99711</td>\n",
              "      <td>0.00251</td>\n",
              "      <td>test-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.99862</td>\n",
              "      <td>0.00027</td>\n",
              "      <td>0.00111</td>\n",
              "      <td>test-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.99726</td>\n",
              "      <td>0.00037</td>\n",
              "      <td>0.00237</td>\n",
              "      <td>test-3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00214</td>\n",
              "      <td>0.99332</td>\n",
              "      <td>0.00454</td>\n",
              "      <td>test-4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.99842</td>\n",
              "      <td>0.00019</td>\n",
              "      <td>0.00139</td>\n",
              "      <td>test-5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>0.99870</td>\n",
              "      <td>0.00013</td>\n",
              "      <td>0.00118</td>\n",
              "      <td>test-1996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>0.99711</td>\n",
              "      <td>0.00068</td>\n",
              "      <td>0.00221</td>\n",
              "      <td>test-1997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>0.97686</td>\n",
              "      <td>0.00038</td>\n",
              "      <td>0.02276</td>\n",
              "      <td>test-1998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>0.99390</td>\n",
              "      <td>0.00366</td>\n",
              "      <td>0.00244</td>\n",
              "      <td>test-1999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>0.99807</td>\n",
              "      <td>0.00011</td>\n",
              "      <td>0.00182</td>\n",
              "      <td>test-2000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af2a220d-0ab6-4d36-a06e-7cf914c8261d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-af2a220d-0ab6-4d36-a06e-7cf914c8261d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-af2a220d-0ab6-4d36-a06e-7cf914c8261d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv(\"gap-test.tsv\", delimiter=\"\\t\")\n",
        "test_data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T10:00:53.662184Z",
          "iopub.execute_input": "2022-05-04T10:00:53.664693Z",
          "iopub.status.idle": "2022-05-04T10:00:53.772164Z",
          "shell.execute_reply.started": "2022-05-04T10:00:53.664646Z",
          "shell.execute_reply": "2022-05-04T10:00:53.770393Z"
        },
        "trusted": true,
        "id": "cB873eja0zMR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "outputId": "6446a10d-ec95-4ab2-f858-7fddc7e3b03a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             ID                                               Text Pronoun  \\\n",
              "0        test-1  Upon their acceptance into the Kontinental Hoc...     His   \n",
              "1        test-2  Between the years 1979-1981, River won four lo...     him   \n",
              "2        test-3  Though his emigration from the country has aff...      He   \n",
              "3        test-4  At the trial, Pisciotta said: ``Those who have...     his   \n",
              "4        test-5  It is about a pair of United States Navy shore...     his   \n",
              "...         ...                                                ...     ...   \n",
              "1995  test-1996  The sole exception was Wimbledon, where she pl...     She   \n",
              "1996  test-1997  According to news reports, both Moore and Fily...     her   \n",
              "1997  test-1998  In June 2009, due to the popularity of the Sab...     She   \n",
              "1998  test-1999  She was delivered to the Norwegian passenger s...     she   \n",
              "1999  test-2000  Meg and Vicky each have three siblings, and ha...     her   \n",
              "\n",
              "      Pronoun-offset                   A  A-offset  A-coref  \\\n",
              "0                383           Bob Suter       352    False   \n",
              "1                430              Alonso       353     True   \n",
              "2                312        Ali Aladhadh       256     True   \n",
              "3                526             Alliata       377    False   \n",
              "4                406               Eddie       421     True   \n",
              "...              ...                 ...       ...      ...   \n",
              "1995             479    Goolagong Cawley       400     True   \n",
              "1996             338  Esther Sheryl Wood       263     True   \n",
              "1997             328               Kayla       364     True   \n",
              "1998             305                Irma       255     True   \n",
              "1999             275        Vicky Austin       217     True   \n",
              "\n",
              "                       B  B-offset  B-coref  \\\n",
              "0                 Dehner       366     True   \n",
              "1     Alfredo Di St*fano       390    False   \n",
              "2                 Saddam       295    False   \n",
              "3              Pisciotta       536     True   \n",
              "4            Rock Reilly       559    False   \n",
              "...                  ...       ...      ...   \n",
              "1995        Peggy Michel       432    False   \n",
              "1996      Barbara Morgan       404    False   \n",
              "1997  Natasha Henstridge       412    False   \n",
              "1998              Bergen       274    False   \n",
              "1999       Polly O'Keefe       260    False   \n",
              "\n",
              "                                                    URL  \n",
              "0            http://en.wikipedia.org/wiki/Jeremy_Dehner  \n",
              "1          http://en.wikipedia.org/wiki/Norberto_Alonso  \n",
              "2                 http://en.wikipedia.org/wiki/Aladhadh  \n",
              "3        http://en.wikipedia.org/wiki/Gaspare_Pisciotta  \n",
              "4                  http://en.wikipedia.org/wiki/Chasers  \n",
              "...                                                 ...  \n",
              "1995  http://en.wikipedia.org/wiki/Evonne_Goolagong_...  \n",
              "1996  http://en.wikipedia.org/wiki/Hastings_Arthur_Wise  \n",
              "1997          http://en.wikipedia.org/wiki/Raya_Meddine  \n",
              "1998        http://en.wikipedia.org/wiki/SS_Irma_(1905)  \n",
              "1999          http://en.wikipedia.org/wiki/Vicky_Austin  \n",
              "\n",
              "[2000 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0d242068-fe0f-46f5-aac0-19f39375d280\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Text</th>\n",
              "      <th>Pronoun</th>\n",
              "      <th>Pronoun-offset</th>\n",
              "      <th>A</th>\n",
              "      <th>A-offset</th>\n",
              "      <th>A-coref</th>\n",
              "      <th>B</th>\n",
              "      <th>B-offset</th>\n",
              "      <th>B-coref</th>\n",
              "      <th>URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test-1</td>\n",
              "      <td>Upon their acceptance into the Kontinental Hoc...</td>\n",
              "      <td>His</td>\n",
              "      <td>383</td>\n",
              "      <td>Bob Suter</td>\n",
              "      <td>352</td>\n",
              "      <td>False</td>\n",
              "      <td>Dehner</td>\n",
              "      <td>366</td>\n",
              "      <td>True</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Jeremy_Dehner</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test-2</td>\n",
              "      <td>Between the years 1979-1981, River won four lo...</td>\n",
              "      <td>him</td>\n",
              "      <td>430</td>\n",
              "      <td>Alonso</td>\n",
              "      <td>353</td>\n",
              "      <td>True</td>\n",
              "      <td>Alfredo Di St*fano</td>\n",
              "      <td>390</td>\n",
              "      <td>False</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Norberto_Alonso</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test-3</td>\n",
              "      <td>Though his emigration from the country has aff...</td>\n",
              "      <td>He</td>\n",
              "      <td>312</td>\n",
              "      <td>Ali Aladhadh</td>\n",
              "      <td>256</td>\n",
              "      <td>True</td>\n",
              "      <td>Saddam</td>\n",
              "      <td>295</td>\n",
              "      <td>False</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Aladhadh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>test-4</td>\n",
              "      <td>At the trial, Pisciotta said: ``Those who have...</td>\n",
              "      <td>his</td>\n",
              "      <td>526</td>\n",
              "      <td>Alliata</td>\n",
              "      <td>377</td>\n",
              "      <td>False</td>\n",
              "      <td>Pisciotta</td>\n",
              "      <td>536</td>\n",
              "      <td>True</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Gaspare_Pisciotta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>test-5</td>\n",
              "      <td>It is about a pair of United States Navy shore...</td>\n",
              "      <td>his</td>\n",
              "      <td>406</td>\n",
              "      <td>Eddie</td>\n",
              "      <td>421</td>\n",
              "      <td>True</td>\n",
              "      <td>Rock Reilly</td>\n",
              "      <td>559</td>\n",
              "      <td>False</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Chasers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>test-1996</td>\n",
              "      <td>The sole exception was Wimbledon, where she pl...</td>\n",
              "      <td>She</td>\n",
              "      <td>479</td>\n",
              "      <td>Goolagong Cawley</td>\n",
              "      <td>400</td>\n",
              "      <td>True</td>\n",
              "      <td>Peggy Michel</td>\n",
              "      <td>432</td>\n",
              "      <td>False</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Evonne_Goolagong_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>test-1997</td>\n",
              "      <td>According to news reports, both Moore and Fily...</td>\n",
              "      <td>her</td>\n",
              "      <td>338</td>\n",
              "      <td>Esther Sheryl Wood</td>\n",
              "      <td>263</td>\n",
              "      <td>True</td>\n",
              "      <td>Barbara Morgan</td>\n",
              "      <td>404</td>\n",
              "      <td>False</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Hastings_Arthur_Wise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>test-1998</td>\n",
              "      <td>In June 2009, due to the popularity of the Sab...</td>\n",
              "      <td>She</td>\n",
              "      <td>328</td>\n",
              "      <td>Kayla</td>\n",
              "      <td>364</td>\n",
              "      <td>True</td>\n",
              "      <td>Natasha Henstridge</td>\n",
              "      <td>412</td>\n",
              "      <td>False</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Raya_Meddine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>test-1999</td>\n",
              "      <td>She was delivered to the Norwegian passenger s...</td>\n",
              "      <td>she</td>\n",
              "      <td>305</td>\n",
              "      <td>Irma</td>\n",
              "      <td>255</td>\n",
              "      <td>True</td>\n",
              "      <td>Bergen</td>\n",
              "      <td>274</td>\n",
              "      <td>False</td>\n",
              "      <td>http://en.wikipedia.org/wiki/SS_Irma_(1905)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>test-2000</td>\n",
              "      <td>Meg and Vicky each have three siblings, and ha...</td>\n",
              "      <td>her</td>\n",
              "      <td>275</td>\n",
              "      <td>Vicky Austin</td>\n",
              "      <td>217</td>\n",
              "      <td>True</td>\n",
              "      <td>Polly O'Keefe</td>\n",
              "      <td>260</td>\n",
              "      <td>False</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Vicky_Austin</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d242068-fe0f-46f5-aac0-19f39375d280')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0d242068-fe0f-46f5-aac0-19f39375d280 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0d242068-fe0f-46f5-aac0-19f39375d280');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actual_vals = []\n",
        "#A=1,B=2,Nei=0\n",
        "for i in range(len(test_data)):\n",
        "  if (test_data.loc[i,\"A-coref\"] == True) and (test_data.loc[i,\"B-coref\"] == False):\n",
        "    actual_vals.append(1)\n",
        "  elif (test_data.loc[i,\"A-coref\"] == False) and (test_data.loc[i,\"B-coref\"] == True):\n",
        "    actual_vals.append(2)\n",
        "  elif (test_data.loc[i,\"A-coref\"] == False) and (test_data.loc[i,\"B-coref\"] == False):\n",
        "    actual_vals.append(0)\n",
        "actual_vals"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T10:00:53.885907Z",
          "iopub.execute_input": "2022-05-04T10:00:53.886562Z",
          "iopub.status.idle": "2022-05-04T10:00:54.024159Z",
          "shell.execute_reply.started": "2022-05-04T10:00:53.886495Z",
          "shell.execute_reply": "2022-05-04T10:00:54.022510Z"
        },
        "trusted": true,
        "id": "fa7ZE0Np0zMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d228acd4-5e40-4cea-e68d-103e71a077b6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_vals = []\n",
        "for i in range(len(df_sub)):\n",
        "    if (df_sub.loc[i,'A']>=df_sub.loc[i,'B']) and (df_sub.loc[i,'A']>=df_sub.loc[i,'NEITHER']):\n",
        "        predicted_vals.append(1)\n",
        "    elif (df_sub.loc[i,'B']>=df_sub.loc[i,'A']) and (df_sub.loc[i,'B']>=df_sub.loc[i,'NEITHER']):\n",
        "        predicted_vals.append(2)\n",
        "    elif (df_sub.loc[i,'NEITHER']>=df_sub.loc[i,'A']) and (df_sub.loc[i,'NEITHER']>=df_sub.loc[i,'B']):\n",
        "        predicted_vals.append(0)\n",
        "print(len(predicted_vals))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T10:00:54.350562Z",
          "iopub.execute_input": "2022-05-04T10:00:54.351112Z",
          "iopub.status.idle": "2022-05-04T10:00:54.531236Z",
          "shell.execute_reply.started": "2022-05-04T10:00:54.351054Z",
          "shell.execute_reply": "2022-05-04T10:00:54.530090Z"
        },
        "trusted": true,
        "id": "-Kkyd-FZ0zMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1ab6854-a481-4466-faf8-1c4042d3bd0a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T10:00:54.740385Z",
          "iopub.execute_input": "2022-05-04T10:00:54.740701Z",
          "iopub.status.idle": "2022-05-04T10:00:54.747966Z",
          "shell.execute_reply.started": "2022-05-04T10:00:54.740632Z",
          "shell.execute_reply": "2022-05-04T10:00:54.746475Z"
        },
        "trusted": true,
        "id": "EEQbkwfi0zMR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicted_vals\n",
        "# actual_vals\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(actual_vals, predicted_vals)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(actual_vals, predicted_vals, average='weighted')\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(actual_vals, predicted_vals, average='weighted')\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(actual_vals, predicted_vals, average='weighted')\n",
        "print('F1 score: %f' % f1)\n",
        " \n",
        "# confusion matrix\n",
        "matrix = confusion_matrix(actual_vals, predicted_vals)\n",
        "print(matrix)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T10:00:55.112592Z",
          "iopub.execute_input": "2022-05-04T10:00:55.112907Z",
          "iopub.status.idle": "2022-05-04T10:00:55.144590Z",
          "shell.execute_reply.started": "2022-05-04T10:00:55.112847Z",
          "shell.execute_reply": "2022-05-04T10:00:55.143474Z"
        },
        "trusted": true,
        "id": "OTill-qh0zMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68a22dd8-35ca-4a94-a37a-b887a8053033"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.830500\n",
            "Precision: 0.830590\n",
            "Recall: 0.830500\n",
            "F1 score: 0.828175\n",
            "[[133  40  54]\n",
            " [ 23 765 130]\n",
            " [ 20  72 763]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "E6XPR2Uk0zMS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}